# 分布式系统
## 经典基础理论
### 系统设计理念
中心化设计：分为两种角色，“领导” 和 “干活的”，强依赖于"领导"节点。
- 问题1：“领导”节点的正常响应问题。可以使用主备方案解决。
- 问题2：“领导”节点的性能瓶颈，影响请求分发。
  
去中心化设计：不是不要中心，而是由节点来自由选择中心。 集群的成员会自发的举行“会议”选举新的“领导”主持工作。最典型的案例就是ZooKeeper及Go语言实现的Etcd（Raft算法）。
- 脑裂问题：指一个集群由于网络的故障，被分为至少两个彼此无法通信的单独集群，此时如果两个集群都各自工作，则可能会产生严重的数据冲突和错误。
- 方案：规模较小的集群就“自杀”或者拒绝服务


## 分布式事务
**分布式事务**（Distributed Transaction）特指多个服务同时访问多个数据源的事务处理机制，请注意它与DTP 模型中“分布式事务”的差异。

### XA 规范
XA 规范里描述了一个 DTP(Distributed Transaction Processing) 模型，这是一个实现分布式事务处理系统的概念模型。

XA 规范里描述了该模型包含三类角色：
- **AP**：Application Program，应用程序，定义了事务的边界，以及指定了组成一个事务的行为，可以理解为就是事务发起的某个微服务。
- **RMs**：Resource Managers，资源管理器，有多个，可以理解为就是分布式数据库中的每一个数据库实例。
- **TM**：Transaction Manager，事务管理器，负责协调和管理事务，是一个控制全局事务的协调者。
> 之所以引入 TM，是因为整个全局事务被分散到多个节点之后，每个节点虽然可以知道自己操作是否成功，但是却无法得知其他节点上操作是否成功，靠分散的节点自身并无法保证全局事务的一致性，因此需要引入一个协调者来管理全局，进而才能保证全局事务的 ACID。

![avatar](https://raw.githubusercontent.com/rbmonster/file-storage/main/learning-note/other/xa.png)

TM 与 RM 之间实现事务的完成和回滚，是使用了 **2PC（Two-Phase Commit）** 协议——即**两阶段提交协议**来实现的。

XA 规范里还定义了一系列接口，称为 XA 接口，用于 TM 和 RM 之间通讯的接口，主要包含了以下这些接口：

![avatar](https://raw.githubusercontent.com/rbmonster/file-storage/main/learning-note/other/xa-description.png)


### CAP理论
- **强一致性（Consistency）**：系统在执行过某项操作后仍然处于一致的状态。在分布式系统中，更新操作执行成功后所有的用户都应该读到最新的值，这样的系统被认为是具有强一致性的。
- **可用性（Availability）**：每一个操作总是能够在一定的时间内返回结果，这里需要注意的是"一定时间内"和"返回结果"。
- **分区容错性（Partition Tolerance）**：理解为在存在网络分区的情况下，仍然可以接受请求（满足一致性和可用性)。这里的网络分区是指由于某种原因，网络被分成若干个孤立的区域，而区域之间互不相通。

**如果放弃可用性（CP without A）**，意味着我们将假设一旦网络发生分区，节点之间的信息同步时间可以无限制地延长，此时，问题相当于退化到前面“全局事务”中一个系统使用多个数据源的场景之中，我们可以通过 2PC/3PC 等手段，同时获得分区容忍性和一致性。
> 在现实中，选择放弃可用性的 CP 系统情况一般用于对数据质量要求很高的场合中，除了 DTP 模型的分布式数据库事务外，著名的 HBase 也是属于 CP 系统，以 HBase 集群为例，假如某个 RegionServer 宕机了，这个 RegionServer 持有的所有键值范围都将离线，直到数据恢复过程完成为止，这个过程要消耗的时间是无法预先估计的。

**如果放弃一致性（AP without C）**，意味着我们将假设一旦发生分区，节点之间所提供的数据可能不一致。选择放弃一致性的 AP 系统目前是设计分布式系统的主流选择，因为 P 是分布式网络的天然属性，你再不想要也无法丢弃；而 A 通常是建设分布式的目的，如果可用性随着节点数量增加反而降低的话，很多分布式系统可能就失去了存在的价值，除非银行、证券这些涉及金钱交易的服务，宁可中断也不能出错，否则多数系统是不能容忍节点越多可用性反而越低的。
> 目前大多数 NoSQL 库和支持分布式的缓存框架都是 AP 系统，以 Redis 集群为例，如果某个 Redis 节点出现网络分区，那仍不妨碍各个节点以自己本地存储的数据对外提供缓存服务，但这时有可能出现请求分配到不同节点时返回给客户端的是不一致的数据。\
> 可横向对比的如注册中心的Zookeeper与Eureka，Zookeeper实现CP，Eureka实现AP

理论相关解释：
[分布式系统之CAP理论](https://www.cnblogs.com/hxsyl/p/4381980.html)

**CAP与注册中心选型**

现在，注册中心的选型有好多种，包括` Zookeeper、Eureka、Consul、Etcd、CoreDNS、Nacos` 等，还可以自研。这么多选择，应该选哪个比较好呢？其实，可以先从 CAP 理论去考虑，本质上，注册中心应该是 CP 模型还是 AP 模型的？

1. 对于服务发现场景来说，即使注册中心的不同节点保存的服务提供者信息不尽相同，也并不会造成灾难性的后果。因为对于服务消费方来说，能消费才是最重要的，拿到不正确的服务提供方节点信息好过因为无法获取服务节点信息而不去消费。
2. 对于服务发现场景来说，针对同一个服务，即使注册中心的不同节点保存的服务提供者信息不尽相同，也并不会造成灾难性的后果。但是对于服务消费者来说，如果因为注册中心的异常导致消费不能正常进行，对于系统来说则是灾难性的。
3. 服务注册的角度分析，如果注册中心发生了网络分区，CP 场景下新的节点无法注册，新部署的服务节点就不能提供服务，站在业务角度这是我们不想看到的，因为我们希望将新的服务节点通知到尽量多的服务消费方，**不能因为注册中心要保证数据的一致性而让所有新节点都不生效**。

综上所述，注册中心应该优先选择 AP 模型，保证高可用，而非强一致性。那以上所罗列出来的注册中心，可选的就只剩下 `Eureka、Nacos，或者自研`了。



### 刚性事务

刚性事务：遵循ACID原则，强一致性。

**2PC** 和 **3PC** 都是一种在分布式环境中仍追求强一致性的事务处理方案，对于多节点而且互相调用彼此服务的场合（典型的就是现在的微服务系统）是极不合适的，今天它几乎只实际应用于单服务多数据源的场合中

#### 两阶段提交 2PC(phase-commit)
![avatar](https://raw.githubusercontent.com/rbmonster/file-storage/main/learning-note/other/2pc.png)

在两阶段提交中，主要涉及到两个角色，分别是协调者和参与者。
1. 第一阶段：当要执行一个分布式事务的时候，事务发起者首先向协调者发起事务请求，然后协调者会给所有参与者发送 prepare 请求（其中包括事务内容）告诉参与者需要执行事务。如果能执行发送的事务内容那么就先执行但不提交，执行后回复。\
   然后参与者收到 prepare 消息后，他们会开始执行事务（但不提交），并将 **Undo 和 Redo 信息记入事务日志**中，之后参与者就向协调者反馈是否准备好了。
2. 第二阶段：提交事务或者回滚事务。比如这个时候 所有的参与者 都返回了准备好了的消息，这个时候就进行事务的提交，协调者此时会给所有的参与者发送 Commit 请求 ，当参与者收到 Commit 请求的时候会执行前面执行的事务的 提交操作 ，提交完毕之后将给协调者发送提交成功的响应。\
   如果在第一阶段有参与者返回了为准备好的消息，那么此时协调者将会给所有参与者发送 回滚事务的 rollback 请求，参与者收到之后将会 回滚它在第一阶段所做的事务处理。

结合XA协议，整个协作的流程如下：

![avatar](https://raw.githubusercontent.com/rbmonster/file-storage/main/learning-note/other/2pc-2.png)


##### 两阶段提交的问题
1. 单点故障问题。如果协调者挂了那么整个系统都处于不可用的状态了，参与者等待协调者指令时无法做超时处理。
2. 性能问题。 即当协调者发送 prepare 请求，参与者收到之后如果能处理，那么它将会进行事务的处理但并不提交，这个时候会一直占用着资源不释放
   - 如果此时协调者挂了，那么这些资源都不会再释放了，这会极大影响性能。
   - 如果正常处理，整个过程将持续到参与者集群中最慢的那一个处理操作结束为止，这决定了两段式提交的性能通常都较差。
3. 数据不一致问题。
   - 协调者宕机。比如当第二阶段，协调者只发送了一部分的 commit 请求就挂了，那么也就意味着，收到消息的参与者会进行事务的提交，而后面没收到的则不会进行事务提交，那么这时候就会产生数据不一致性问题。
   - 脑裂问题。如果分布式节点出现网络分区，某些参与者未收到commit提交命令，就会出现一部分提交数据，而另一部分未提交数据的不一致问题。
4. 网络传输导致空回滚和资源悬挂
   - 空回滚：没收到 prepare 请求，但收到了 rollback 请求，那这个 rollback 请求其实是无效的，即本次 rollback 就属于空回滚。
   - 资源悬挂：如果 prepare 请求因为网络拥堵而超时，之后 TM 发起了 rollback，而最终 RM 又收到了超时的 prepare 请求，但 rollback 比 prepare 先到达 RM。

2PC 最大的问题其实是性能差，处理短事务可能还好，要是处理长事务，那资源锁定时间更长，性能更差，根本无法忍受。


#### 三阶段提交 3PC(phase-commit)
![avatar](https://raw.githubusercontent.com/rbmonster/file-storage/main/learning-note/learning/basic/3PC.jpg)

三阶段提交的流程如下：
1. CanCommit阶段：协调者向所有参与者发送 CanCommit 请求，参与者收到请求后会根据自身情况查看是否能执行事务，如果可以则返回 YES 响应并进入预备状态，否则返回 NO 。
2. PreCommit阶段：协调者根据参与者返回的响应来决定是否可以进行下面的 PreCommit 操作。
   1. 如果上面参与者返回的都是 YES，那么协调者将向所有参与者发送 PreCommit 预提交请求，参与者收到预提交请求后，会进行事务的执行操作，并将 Undo 和 Redo 信息写入事务日志中 ，最后如果参与者顺利执行了事务则给协调者返回成功的响应。
   2. 如果在第一阶段协调者收到了 任何一个 NO 的信息，或者 在一定时间内 并没有收到全部的参与者的响应，那么就会中断事务，它会向所有参与者发送中断请求（abort）
3. DoCommit阶段：如果协调者收到了所有参与者在 PreCommit 阶段的 YES 响应，那么协调者将会给所有参与者发送 DoCommit 请求，**参与者收到 DoCommit 请求后则会进行事务的提交工作**，完成后则会给协调者返回响应，协调者收到所有参与者返回的事务提交成功的响应之后则完成事务。若协调者在 PreCommit 阶段 收到了任何一个 NO 或者在一定时间内没有收到所有参与者的响应 ，那么就会进行中断请求的发送，参与者收到中断请求后则会 通过上面记录的回滚日志 来进行事务的回滚操作，并向协调者反馈回滚状况，协调者收到参与者返回的消息后，中断事务。
> 3PC 在 DoCommit 阶段参与者如未收到协调者发送的提交事务的请求，它会在一定时间内进行事务的提交。因为这个时候我们肯定保证了在第一阶段所有的协调者全部返回了可以执行事务的响应，这个时候我们有理由相信其他系统都能进行事务的执行和提交


总结：3PC 通过一系列的超时机制很好的缓解了阻塞问题，相比2PC参与者也有了超时中断机制。解决了无限阻塞及单点故障问题，但是仍然无法解决网络分区问题。

缺点： 未解决网络分区问题，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。

**3PC 给 RMs 也增加了超时机制，而且把整个事务拆成了三个阶段。不过，3PC 也只是解决了 2PC 的部分问题，并没有解决性能差的问题，而且因为多增加了一个阶段，导致性能更差了。**

### 柔性事务
柔性事务：遵循BASE理论，最终一致性；与刚性事务不同，柔性事务允许一定时间内，不同节点的数据不一致，但要求最终一致。

刚性事务，主要保证强一致性。XA/2PC 就是解决刚性分布式事务的主要方案，**但因为性能太差**，并不适合高性能、高并发的互联网场景。为了解决性能问题，就有人基于 BASE 理论提出了柔性事务的概念。

#### BASE理论
BASE 是Basically Available（基本可用） 、Soft-state（软状态，柔性事务） 和 Eventually Consistent（最终一致性）
- **基本可用**是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。
- **软状态**(柔性事务)指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。
- **最终一致性**强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。

**即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency)**。

#### TCC事务(Try-Confirm-Cancel)
可靠消息队列虽然能保证最终的结果是相对可靠的，过程也足够简单（相对于 TCC 来说），但整个过程完全没有任何隔离性可言，缺乏隔离性会带来的一个显而易见的问题便是“超售”。

TCC 它是一种业务侵入式较强的事务方案，要求业务处理过程必须拆分为“预留业务资源”和“确认/释放消费资源”两个子过程。 具体分为以下三个阶段:
- **Try**：尝试执行阶段，完成所有业务可执行性的检查（保障一致性），并且预留好全部需用到的业务资源（保障隔离性）。
- **Confirm**：确认执行阶段，不进行任何业务检查，直接使用 Try 阶段准备的资源来完成业务处理。Confirm 阶段可能会重复执行，因此本阶段所执行的操作需要具备幂等性。
- **Cancel**：取消执行阶段，释放 Try 阶段预留的业务资源。Cancel 阶段可能会重复执行，也需要满足幂等性。


如上述购书的例子：
1. 购买书籍，分别去账户服务冻结余额、仓库服务冻结库存、商家服务生成预收款，处理成功进入confirm阶段
2. 三个操作都进入confirm阶段，进行实际的业务操作：账务服务扣减余额、仓库扣减库存、商家服务支付首款
3. 三个操作有一个操作超时或者操作失败，进行cancel操作，取消资源预留操作。


TCC 其实也是基于 **2PC 的设计思路**演变过来的，也同样分两个阶段进行事务提交，第一阶段提交 Try 接口，第二阶段提交 Confirm 或 Cancel 接口。
- 2PC 其实是数据库层面或者说是资源层面的分布式事务方案，Prepare-Commit-Rollback，这几个操作其实都在数据库内部完成的，开发者层面是感知不到的。
- TCC 则是业务层面的分布式事务方案，Try-Confirm-Cancel 都是在业务层面实现的操作，开发者是能感知到的，是需要开发者自己去实现这几个操作的。

> TCC 其实有点类似 2PC 的准备阶段和提交阶段，但 TCC 是位于用户代码层面，而不是在基础设施层面，这为它的实现带来了较高的灵活性，可以根据需要设计资源锁定的粒度。\
TCC 在业务执行时只操作预留资源，几乎不会涉及锁和资源的争用，具有很高的性能潜力。但是 TCC 并非纯粹只有好处，它也带来了更高的开发成本和业务侵入性，**意味着有更高的开发成本和更换事务实现方案的替换成本**。

**TCC 落地实现的开源框架主要有 ByteTCC、TCC-transaction、Himly、Seata TCC 模式等。**

#### SAGA事务
在分布式事务中，若存在与外部交互的系统，那么就不可能使用TCC的冻结或预留等Try阶段操作。\
SAGA事务是一个分布式环境中的大事务分解为一系列本地事务的设计模式。
- 大事务拆分若干个小事务，将整个分布式事务 T 分解为 n 个子事务，命名为 T1，T2，…，Ti，…，Tn。每个子事务都应该是或者能被视为是原子行为。如果分布式事务能够正常提交，其对数据的影响（最终一致性）应与连续按顺序成功提交 Ti等价。
- 为每一个子事务设计对应的补偿动作，命名为 C1，C2，…，Ci，…，Cn。Ti与 Ci必须满足以下条件：
  1. Ti与 Ci都具备幂等性。
  2. Ti与 Ci满足交换律（Commutative），即先执行 Ti还是先执行 Ci，其效果都是一样的。
  3. Ci必须能成功提交，即不考虑 Ci本身提交失败被回滚的情形，如出现就必须持续重试直至成功，或者要人工介入。

最佳情况就是整个子事务序列 T1, T2, ..., Tn 全部都执行成功，整个 Saga 事务也就执行成功了。 否则，要采取以下两种恢复策略之一：
- 正向恢复（Forward Recovery）：重试失败的事务，假设每个子事务最终都会成功。如果 Ti事务提交失败，则一直对 Ti进行重试，直至成功为止（最大努力交付）。
    > 这种恢复方式不需要补偿，适用于事务最终都要成功的场景，譬如在别人的银行账号中扣了款，就一定要给别人发货。正向恢复的执行模式为：T1，T2，…，Ti（失败），Ti（重试）…，Ti+1，…，Tn。
- 反向恢复（Backward Recovery）：补偿所有已完成的事务，本质就是所有已完成的本地事务进行回滚操作。如果 Ti事务提交失败，则一直执行 Ci对 Ti进行补偿，直至成功为止（最大努力交付）。
    > 这里要求 Ci必须（在持续重试后）执行成功。反向恢复的执行模式为：T1，T2，…，Ti（失败），Ci（补偿），…，C2，C1。

与 TCC 相比，SAGA 不需要为资源设计冻结状态和撤销冻结的操作，补偿操作往往要比冻结操作容易实现得多。SAGA 事务通常也不会直接靠裸编码来实现，一般也是在事务中间件的基础上完成，如阿里巴巴的Seata。

Saga 的实现方式，主要分**集中式**和**非集中式**两种: 

集中式的实现需要依赖中心化的协调器（TM）负责服务调用和事务协调，主要是基于 AOP Proxy 的设计实现，华为的 ServiceComb Saga 就用这种实现方式。
> 集中式的实现方式比较直观并且容易控制，开发简单、学习成本低，缺点就是业务耦合程度会比较高。

非集中式的实现，也称为分布式的实现，不依赖于中心化的 TM，而是通过事件驱动的机制进行事务协调，Seata Saga 就采用了这种机制，实现了一个状态机。
> - 优点：采用事件源的方式降低系统复杂程度，提升系统扩展性， 处理模块通过订阅事件的方式降低系统的耦合程度。
> - 缺点：Saga 系统会涉及大量的业务事件，这样会对编码和调试带来一些问题；还有就是相关的业务逻辑处理是基于事件，相关事件处理模块可能会有循环依赖的问题。

#### 事务消息型
事务消息型，也称异步确保型，核心思路就是：用消息队列（MQ）来保证最终一致性。相比同步的补偿型方案，引入 MQ 的异步方案，主要有以下优点：
- 可以降低不同分支事务的微服务之间的耦合度
- 可以提高各服务的吞吐量
- 可以增强整体服务的可用性

引入 MQ 之后，最核心的问题在于如何解决服务本地事务处理成功与消息发送成功两者的一致性问题。即 MQ 消息的上游服务处理完本地事务之后，如何才能保证消息可靠地传递给到下游服务。而目前业界解决该问题的方案有两种：
- 基于 MQ 自身的事务消息
- 基于 DB 的本地消息表

##### MQ 事务消息
工作流程:

![avatar](https://raw.githubusercontent.com/rbmonster/file-storage/main/learning-note/other/middleware/rocketMqTransaction.jpg)

相关概念：
- **事务消息**：消息队列 MQ 提供类似 X/Open XA 的分布式事务功能，通过 MQ 事务消息能达到分布式事务的最终一致。
- **半事务消息**：暂不能投递的消息，发送方已经成功地将消息发送到了 MQ 服务端，但是服务端未收到生产者对该消息的二次确认，此时该消息被标记成“暂不能投递”状态，处于该种状态下的消息即半事务消息。
- **消息回查**：由于网络闪断、生产者应用重启等原因，导致某条事务消息的二次确认丢失，MQ 服务端通过扫描发现某条消息长期处于“半事务消息”时，需要主动向消息生产者询问该消息的最终状态（Commit或是Rollback），该询问过程即消息回查。

> 如果发送方没有及时收到 MQ 服务端的 Ack 结果，那就可能造成 MQ 消息的重复投递，因此，订阅方必须对消息的消费做**幂等处理**，不能造成同一条消息重复消费的情况。

MQ 事务消息方案的**最大缺点就是对业务具有侵入性**，业务发送方需要提供回查接口。

##### 本地消息表

对于不支持事务消息的 MQ 则可以采用此方案，其核心的设计思路就是将**事务消息存储到本地数据库**中，并且消息数据的记录与业务数据的记录**必须在同一个事务**内完成。将消息数据保存到 DB 之后，就可以通过一个定时任务到 DB 中去轮询查出状态为待发送的消息，然后将消息投递给 MQ，成功收到 MQ 的 ACK 确认之后，再将 DB 中消息的状态更新或者删除消息。

本地消息表这个方案最初是ebay提出的[ebay的完整方案](https://queue.acm.org/detail.cfm?id=1394128)

![avatar](https://raw.githubusercontent.com/rbmonster/file-storage/main/learning-note/other/txn-local-message.png)

处理步骤如下：
1. 消息生产者在本地事务中处理业务更新操作，并写一条事务消息到本地消息表，该消息的状态为待发送，业务操作和写消息表都在同一个本地事务中完成。
2. 定时任务不断轮询从本地消息表中查询出状态为待发送状态的消息，并将查出的所有消息投递到 MQ Server。
3. MQ Server 接收到消息之后，就会将消息进行持久化，然后返回 ACK 给到消息生产者。
4. 消息生产者收到了 MQ Server 的 ACK 之后，再从本地消息表中查询出对应的消息记录，将消息的状态更新为已发送，或者直接删除消息记录。
5. MQ Server 返回 ACK 给到消息生产者之后，接着就会将消息发送给消息消费者。
6. 消息消费者接收到消息之后，执行本地事务，最后返回 ACK 给到 MQ Server。
因为 MQ 宕机或网络中断等原因，生产者有可能会向 MQ 发送重复消息，因此，消费者接收消息后的处理需要支持**幂等**。


**优点**: \
相比 MQ 事务消息方案，其优点就是弱化了对 MQ 的依赖，因为消息数据的可靠性依赖于本地消息表，而不依赖于 MQ。 还有一个优点就是容易实现。

**缺点**: 
- 本地消息表与业务耦合在一起，难以做成通用性，且消息数据与业务数据同个数据库，占用了业务系统资源。
- 本地消息表是基于数据库来做的，而数据库是要读写磁盘 I/O 的，因此在高并发下是有性能瓶颈的。



#### 可靠事件队列(最大努力交付)
定义：靠着持续重试来保证可靠性的分布式事务最终一致性解决方案

缺点：过程无隔离性，缺乏隔离性会带来的一个显而易见的问题便是“超售”：完全有可能两个客户在短时间内都成功购买了同一件商品，而且他们各自购买的数量都不超过目前的库存，但他们购买的数量之和却超过了库存。
![avatar](https://raw.githubusercontent.com/rbmonster/file-storage/main/learning-note/other/basicTimingDiagram.png)

在系统中建立一个消息服务，定时轮询消息表，将状态是“进行中”的消息同时发送到库存和商家服务节点中去。这时候可能产生以下几种情况。
1. 商家和仓库服务都成功完成了收款和出库工作，向用户账号服务器返回执行结果，用户账号服务把消息状态从“进行中”更新为“已完成”。整个事务宣告顺利结束，达到最终一致性的状态。
2. 商家或仓库服务中至少一个因网络原因，未能收到来自用户账号服务的消息。此时，由于用户账号服务器中存储的消息状态一直处于“进行中”，所以消息服务器将在每次轮询的时候持续地向未响应的服务重复发送消息。这个步骤的可重复性决定了所有被消息服务器发送的消息都必须**具备幂等性**，通常的设计是让消息带上一个**唯一的事务 ID**，以保证一个事务中的出库、收款动作会且只会被处理一次。
3. 商家或仓库服务有某个或全部无法完成工作，譬如仓库发现《深入理解 Java 虚拟机》没有库存了，此时，仍然是持续自动重发消息，直至操作成功（譬如补充了新库存），或者被人工介入为止。由此可见，可靠事件队列只要第一步业务完成了，后续就没有失败回滚的概念，只许成功，不许失败。
4. 商家和仓库服务成功完成了收款和出库工作，但回复的应答消息因网络原因丢失，此时，用户账号服务仍会重新发出下一条消息，但因操作具备幂等性，所以不会导致重复出库和收款，只会导致商家、仓库服务器重新发送一条应答消息，此过程重复直至双方网络通信恢复正常。

也有一些支持分布式事务的消息框架，如 RocketMQ，原生就支持分布式事务操作，这时候上述情况 2、4 也可以交由消息框架来保障。\
以上这种是靠着**持续重试来保证可靠性**的解决方案，它在计算机的其他领域中已被频繁使用，也有了专门的名字叫作“**最大努力交付**”（Best-Effort Delivery），譬如 TCP 协议中未收到 ACK 应答自动重新发包的可靠性保障就属于最大努力交付。

### 分布式事务方案选型

| 属性| 	XA/2PC| 	TCC	Saga| 	MQ事务消息| 	本地消息表| 	最大努力通知型|
| --- | --- | ---|  ---| ---| ---| 
| 事务一致性| 	强| 	中| 	弱| 	弱| 	弱| 	弱|
| 性能| 	低| 	中| 	高| 	高| 	高| 	高|
| 业务侵入性| 	小| 	大| 	小| 	中| 	中| 	中|
| 复杂性| 	中| 	高| 	中| 	低| 	低| 	低|
| 维护成本| 	低| 	高| 	中| 	中| 	低| 	中|

具体如何选型，其实还是需要根据场景而定。**我们应该由场景驱动架构，离开场景谈架构就是耍流氓**。

1. 最大努力通知型: 如果是要解决和外部第三方系统的业务交互，比如交易系统对接了第三方支付系统，那我们就只能选择最大努力通知型。
2. XA/2PC: 如果对强一致性有刚性要求的短事务，对高性能和高并发则没要求的场景，那可以考虑用 XA/2PC，如果是用 Java 的话，那落地实现可以直接用 Seata 框架的 XA 模式。
3. TCC: 如果对一致性要求高，实时性要求也高，执行时间确定且较短的场景，就比较适合用 TCC，比如用在互联网金融的交易、支付、账务事务。落地实现如果是 Java 也建议可以直接用 Seata 的 TCC 模式。
4. Saga: Saga 则适合于业务场景事务并发操作同一资源较少的情况，因为 Saga 本身不能保证隔离性。而且，Saga 没有预留资源的动作，所以补偿动作最好也是容易处理的场景。
5. MQ 事务消息和本地消息表方案适用于异步事务，对一致性的要求比较低，业务上能容忍较长时间的数据不一致，事务涉及的参与方和参与环节也较少，且业务上还**有对账/校验系统兜底**。如果系统中用到了 RocketMQ，那就可以考虑用 MQ 事务消息方案，因为 MQ 事务消息方案目前只有 RocketMQ 支持。否则，那就考虑用本地消息表方案。
> 最优雅的方案 **业务规避**，意思就是说可以从业务上稍作调整，从而规避掉分布式事务，这是解决分布式事务问题最优雅的方案


**案例**：
下单这个业务的分布式事务处理方案。

下单其实存在三个步骤：
1. 创建订单；
2. 冻结用户的资产账户余额；
3. 将订单投递给到撮合引擎进行撮合。
下单事务的发起方是交易服务，第一步也是在交易服务完成的，而第二步应该是在公共服务完成的——因为我们还没有将账户服务抽离出来——第三步则是通过 MQ 将订单投递给到撮合引擎。

从上面提到的场景分类来说，我们的交易场景属于互联网金融的交易事务，那比较适合用 TCC，但最后一步又是异步事务，这又该怎么选呢？其实，前两步用 TCC 保证同步事务的一致性，而第三步用本地消息表来异步确保消息的可靠投递，这样的处理是可以的。但必须前两步的事务执行成功后，才把消息写入消息表。而撮合引擎作为 MQ 的消费者，就需要做幂等处理了。
> 一个事务有时候并非就只能用一种单一的方案，可以组合，可以演变的。分布式事务问题之所以复杂，最根本的原因也在此，现实场景远比理论复杂多变。

### seata 阿里巴巴
分布式事务解决方案：
[Seata](https://seata.io/zh-cn/index.html)

### 参考资料
- [交易系统架构演进之路（四）：分布式事务](https://mp.weixin.qq.com/s?__biz=MzA5OTI1NDE0Mw==&mid=2652494127&idx=1&sn=2dbd3d6da7e513f3eeadb9db85f3f6c6&chksm=8b68533fbc1fda296bc4170c59e2c6abbe0baa0d4205d38bbca1f4c0b8f3acbddcc0d78a3571&cur_album_id=1626798248214495243&scene=189#wechat_redirect)
- [凤凰架构-分布式事务](https://icyfenix.cn/architect-perspective/general-architecture/transaction/distributed.html)


## 分布式共识算法
共识（Consensus）与一致性（Consistency）的区别：一致性是指数据不同副本之间的差异，而共识是指达成一致性的方法与过程。

### Raft算法
算法：主要用来竞选主节点。
- [Raft算法图解](http://thesecretlivesofdata.com/raft/)

该算法定义了三种节点：Follower、Candidate 和 Leader。
- Leader 会周期性的发送心跳包给 Follower。
- 每个 Follower 都设置了一个随机的竞选超时时间，一般为 150ms~300ms，如果在这个时间内没有收到 Leader 的心跳包，就会变成 Candidate，进入竞选阶段。

#### 选举流程
1. 一个分布式系统的最初阶段，此时只有 Follower 没有 Leader。Node A 等待一个随机的竞选超时时间之后，没收到 Leader 发来的心跳包，因此进入竞选阶段。
2. Node A 发送投票请求给其它所有节点。其它节点会对请求进行回复，如果超过一半的节点回复了，那么该 Candidate 就会变成 Leader。
3. leader 会周期性地发送心跳包给 Follower，Follower 接收到心跳包，会重新开始计时。
4. 针对多候选人选举的情况：
   1. 如果有多个 Follower 成为 Candidate，并且所获得票数相同，那么就需要重新开始投票。
   2. 由于每个节点设置的随机竞选超时时间不同，因此下一次再次出现多个 Candidate 并获得同样票数的概率很低。
   
#### 数据同步
1. 自客户端的修改都会被传入 Leader。注意该修改还未被提交，只是写入日志中。
2. Leader 会把修改复制到所有 Follower。
3. Leader 会等待大多数的 Follower 也进行了修改，然后才将修改提交。
4. 此时 Leader 会通知的所有 Follower 让它们也提交修改，此时所有节点的值达成一致。


### paxos算法
三阶段提交的改进算法

TODO
